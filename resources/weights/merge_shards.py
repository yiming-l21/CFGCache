#!/usr/bin/env python3
"""
FLUX.1-dev åˆ†ç‰‡æ–‡ä»¶åˆå¹¶è„šæœ¬
å°† transformer/ ç›®å½•ä¸‹çš„åˆ†ç‰‡æ–‡ä»¶åˆå¹¶ä¸ºå•ä¸ª flux1-dev.safetensors æ–‡ä»¶
"""

import json
import os
import time
import gc
import psutil
from pathlib import Path
from safetensors.torch import load_file, save_file
import torch

def format_size(bytes_size):
    """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°æ˜¾ç¤º"""
    for unit in ['B', 'KB', 'MB', 'GB']:
        if bytes_size < 1024.0:
            return f"{bytes_size:.1f}{unit}"
        bytes_size /= 1024.0
    return f"{bytes_size:.1f}TB"

def get_memory_info():
    """è·å–å½“å‰å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    try:
        process = psutil.Process()
        memory_info = process.memory_info()
        system_memory = psutil.virtual_memory()
        
        return {
            'rss_gb': memory_info.rss / (1024**3),
            'available_gb': system_memory.available / (1024**3),
            'percent': system_memory.percent
        }
    except:
        return None

def clear_memory():
    """å¼ºåˆ¶æ¸…ç†å†…å­˜"""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

def check_disk_space(output_file, required_gb):
    """æ£€æŸ¥ç£ç›˜ç©ºé—´æ˜¯å¦è¶³å¤Ÿ"""
    output_dir = Path(output_file).parent
    try:
        stat = os.statvfs(output_dir)
        free_bytes = stat.f_bavail * stat.f_frsize
        free_gb = free_bytes / (1024**3)
        
        print(f"ğŸ’¾ ç£ç›˜å¯ç”¨ç©ºé—´: {free_gb:.1f}GB")
        print(f"ğŸ“ éœ€è¦ç©ºé—´: {required_gb:.1f}GB")
        
        if free_gb < required_gb + 1:  # é¢å¤–ç•™1GBç¼“å†²
            print(f"âš ï¸  è­¦å‘Š: ç£ç›˜ç©ºé—´å¯èƒ½ä¸è¶³!")
            return False
        return True
    except:
        print("âš ï¸  æ— æ³•æ£€æŸ¥ç£ç›˜ç©ºé—´")
        return True

def merge_flux_shards(model_dir: str, force: bool = False, resume: bool = True):
    """åˆå¹¶ FLUX åˆ†ç‰‡æ–‡ä»¶"""
    start_time = time.time()
    
    model_path = Path(model_dir)
    transformer_dir = model_path / "transformer"
    index_file = transformer_dir / "diffusion_pytorch_model.safetensors.index.json"
    output_file = model_path / "flux1-dev.safetensors"
    temp_dir = model_path / ".merge_temp"
    progress_file = temp_dir / "merge_progress.json"
    
    print("ğŸš€ FLUX.1-dev åˆ†ç‰‡åˆå¹¶è„šæœ¬")
    print("=" * 50)
    print(f"ğŸ“ æ¨¡å‹ç›®å½•: {model_path}")
    print(f"ğŸ“‚ Transformerç›®å½•: {transformer_dir}")
    print(f"ğŸ“‹ ç´¢å¼•æ–‡ä»¶: {index_file}")
    print(f"ğŸ“¤ è¾“å‡ºæ–‡ä»¶: {output_file}")
    print("=" * 50)
    
    # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
    if output_file.exists() and not force:
        print(f"âœ… å‘ç°å·²å­˜åœ¨çš„åˆå¹¶æ–‡ä»¶: {output_file}")
        size_mb = output_file.stat().st_size / (1024 * 1024)
        print(f"ğŸ“ æ–‡ä»¶å¤§å°: {size_mb:.1f} MB")
        
        choice = input("\næ˜¯å¦é‡æ–°åˆå¹¶? (y/N): ").strip().lower()
        if choice not in ['y', 'yes']:
            print("ğŸ”„ ä½¿ç”¨ç°æœ‰æ–‡ä»¶ï¼Œè·³è¿‡åˆå¹¶")
            return str(output_file)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æœªå®Œæˆçš„åˆå¹¶è¿›åº¦
    completed_shards = set()
    if resume and progress_file.exists():
        try:
            with open(progress_file, 'r') as f:
                progress_data = json.load(f)
                completed_shards = set(progress_data.get('completed_shards', []))
                if completed_shards:
                    print(f"ğŸ”„ å‘ç°æœªå®Œæˆçš„åˆå¹¶è¿›åº¦: {len(completed_shards)} ä¸ªåˆ†ç‰‡å·²å®Œæˆ")
        except:
            print("âš ï¸  æ— æ³•è¯»å–è¿›åº¦æ–‡ä»¶ï¼Œå°†é‡æ–°å¼€å§‹")
            completed_shards = set()
    
    # æ£€æŸ¥å¿…è¦æ–‡ä»¶
    if not index_file.exists():
        raise FileNotFoundError(f"âŒ ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {index_file}")
    
    if not transformer_dir.exists():
        raise FileNotFoundError(f"âŒ Transformerç›®å½•ä¸å­˜åœ¨: {transformer_dir}")
    
    # è¯»å–ç´¢å¼•æ–‡ä»¶
    print("ğŸ“– è¯»å–ç´¢å¼•æ–‡ä»¶...")
    with open(index_file, 'r') as f:
        index_data = json.load(f)
    
    weight_map = index_data.get('weight_map', {})
    if not weight_map:
        raise ValueError("âŒ ç´¢å¼•æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ° weight_map")
    
    # è·å–æ‰€æœ‰åˆ†ç‰‡æ–‡ä»¶
    shard_files = sorted(set(weight_map.values()))
    print(f"ğŸ” å‘ç° {len(shard_files)} ä¸ªåˆ†ç‰‡æ–‡ä»¶:")
    
    total_size = 0
    for shard in shard_files:
        shard_path = transformer_dir / shard
        if not shard_path.exists():
            raise FileNotFoundError(f"âŒ åˆ†ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {shard_path}")
        
        size = shard_path.stat().st_size
        total_size += size
        print(f"  ğŸ“¦ {shard} - {format_size(size)}")
    
    print(f"ğŸ“Š åˆ†ç‰‡æ–‡ä»¶æ€»å¤§å°: {format_size(total_size)}")
    
    # æ£€æŸ¥ç£ç›˜ç©ºé—´
    required_gb = total_size / (1024**3)
    if not check_disk_space(output_file, required_gb):
        choice = input("\nâš ï¸  ç£ç›˜ç©ºé—´ä¸è¶³ï¼Œæ˜¯å¦ç»§ç»­? (y/N): ").strip().lower()
        if choice not in ['y', 'yes']:
            print("âŒ ç”¨æˆ·å–æ¶ˆæ“ä½œ")
            return None
    
    print(f"\nğŸ”„ å¼€å§‹åˆå¹¶ {len(weight_map)} ä¸ªæƒé‡å‚æ•°...")
    
    # æ˜¾ç¤ºåˆå§‹å†…å­˜çŠ¶æ€
    mem_info = get_memory_info()
    if mem_info:
        print(f"ğŸ’¾ åˆå§‹å†…å­˜: {mem_info['rss_gb']:.1f}GB ä½¿ç”¨, {mem_info['available_gb']:.1f}GB å¯ç”¨")
    
    # ä½¿ç”¨åˆ†æ‰¹æµå¼å¤„ç†é¿å…å†…å­˜æº¢å‡º
    merged_state_dict = {}
    batch_size = 100  # æ¯æ‰¹å¤„ç†æƒé‡æ•°é‡
    total_weights = len(weight_map)
    
    # æŒ‰åˆ†ç‰‡ç»„ç»‡æƒé‡ï¼Œå‡å°‘é‡å¤åŠ è½½
    shard_to_weights = {}
    for weight_name, shard_file in weight_map.items():
        if shard_file not in shard_to_weights:
            shard_to_weights[shard_file] = []
        shard_to_weights[shard_file].append(weight_name)
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•ç”¨äºä¿å­˜è¿›åº¦
    temp_dir.mkdir(exist_ok=True)
    
    processed_weights = 0
    
    # é€ä¸ªåˆ†ç‰‡å¤„ç†
    for shard_idx, (shard_file, weight_names) in enumerate(shard_to_weights.items()):
        # æ£€æŸ¥æ˜¯å¦å·²ç»å¤„ç†è¿‡è¿™ä¸ªåˆ†ç‰‡
        if shard_file in completed_shards:
            print(f"  â­ï¸  è·³è¿‡å·²å®Œæˆçš„åˆ†ç‰‡: {shard_file}")
            processed_weights += len(weight_names)
            continue
            
        shard_path = transformer_dir / shard_file
        print(f"  ğŸ“¥ å¤„ç†åˆ†ç‰‡ {shard_idx+1}/{len(shard_to_weights)}: {shard_file}")
        
        # æ˜¾ç¤ºå†…å­˜çŠ¶æ€
        mem_info = get_memory_info()
        if mem_info:
            print(f"      ğŸ’¾ å†…å­˜çŠ¶æ€: {mem_info['rss_gb']:.1f}GB ä½¿ç”¨, {mem_info['available_gb']:.1f}GB å¯ç”¨ ({mem_info['percent']:.1f}%)")
        
        shard_start = time.time()
        
        try:
            # åŠ è½½åˆ†ç‰‡
            shard_data = load_file(str(shard_path))
            
            # åˆ†æ‰¹å¤„ç†æƒé‡
            for i in range(0, len(weight_names), batch_size):
                batch_weights = weight_names[i:i+batch_size]
                
                for weight_name in batch_weights:
                    if weight_name in shard_data:
                        merged_state_dict[weight_name] = shard_data[weight_name]
                    else:
                        print(f"âš ï¸  è­¦å‘Š: æƒé‡ {weight_name} åœ¨åˆ†ç‰‡ {shard_file} ä¸­æœªæ‰¾åˆ°")
                    
                    processed_weights += 1
                
                # å®šæœŸæ˜¾ç¤ºè¿›åº¦å’Œæ¸…ç†å†…å­˜
                if processed_weights % 500 == 0:
                    progress = processed_weights / total_weights * 100
                    print(f"      ğŸ“ˆ è¿›åº¦: {processed_weights}/{total_weights} ({progress:.1f}%)")
                    clear_memory()
            
            # æ¸…ç†åˆ†ç‰‡æ•°æ®
            del shard_data
            clear_memory()
            
            # è®°å½•å·²å®Œæˆçš„åˆ†ç‰‡
            completed_shards.add(shard_file)
            try:
                with open(progress_file, 'w') as f:
                    json.dump({'completed_shards': list(completed_shards)}, f)
            except:
                pass  # å¿½ç•¥è¿›åº¦ä¿å­˜é”™è¯¯
            
            shard_elapsed = time.time() - shard_start
            print(f"      âœ“ å®Œæˆ ({shard_elapsed:.1f}ç§’) - {len(weight_names)} ä¸ªæƒé‡")
            
        except Exception as e:
            print(f"      âŒ åŠ è½½åˆ†ç‰‡å¤±è´¥: {e}")
            # ä¿å­˜å½“å‰è¿›åº¦
            try:
                with open(progress_file, 'w') as f:
                    json.dump({'completed_shards': list(completed_shards)}, f)
                print(f"ğŸ’¾ å·²ä¿å­˜è¿›åº¦åˆ°: {progress_file}")
            except:
                pass
            raise
    
    print(f"\nâœ… æƒé‡åˆå¹¶å®Œæˆ! æ€»å…± {len(merged_state_dict)} ä¸ªå‚æ•°")
    
    # æ˜¾ç¤ºæœ€ç»ˆå†…å­˜çŠ¶æ€
    mem_info = get_memory_info()
    if mem_info:
        print(f"ğŸ’¾ åˆå¹¶åå†…å­˜: {mem_info['rss_gb']:.1f}GB ä½¿ç”¨, {mem_info['available_gb']:.1f}GB å¯ç”¨")
    
    # è®¡ç®—åˆå¹¶åå¤§å°
    merged_size = sum(tensor.numel() * tensor.element_size() for tensor in merged_state_dict.values())
    print(f"ğŸ“ åˆå¹¶åå¤§å°: {format_size(merged_size)}")
    
    # ä¿å­˜åˆå¹¶åçš„æ–‡ä»¶ - åˆ†æ‰¹ä¿å­˜ä»¥å‡å°‘å†…å­˜å‹åŠ›
    print(f"\nğŸ’¾ ä¿å­˜åˆå¹¶æ–‡ä»¶åˆ°: {output_file}")
    save_start = time.time()
    
    # æ£€æŸ¥å†…å­˜æƒ…å†µï¼Œå¦‚æœå†…å­˜ç´§å¼ å°±å…ˆæ¸…ç†
    mem_info = get_memory_info()
    if mem_info and mem_info['available_gb'] < 5:  # å¯ç”¨å†…å­˜å°‘äº5GB
        print("âš ï¸  å†…å­˜ç´§å¼ ï¼Œæ‰§è¡Œåƒåœ¾å›æ”¶...")
        clear_memory()
    
    try:
        save_file(merged_state_dict, str(output_file))
        save_elapsed = time.time() - save_start
        print(f"âœ… ä¿å­˜å®Œæˆ ({save_elapsed:.1f}ç§’)")
        
    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
        print("ğŸ’¡ å»ºè®®: å°è¯•é‡Šæ”¾æ›´å¤šå†…å­˜æˆ–ä½¿ç”¨æ›´å¤§å†…å­˜çš„æœºå™¨")
        raise
    
    # éªŒè¯è¾“å‡ºæ–‡ä»¶
    if output_file.exists():
        output_size = output_file.stat().st_size
        print(f"ğŸ“Š è¾“å‡ºæ–‡ä»¶å¤§å°: {format_size(output_size)}")
        
        # è®¡ç®—å‹ç¼©æ¯”
        compression_ratio = (total_size - output_size) / total_size * 100
        print(f"ğŸ“‰ ç©ºé—´ä¼˜åŒ–: {compression_ratio:.1f}% (èŠ‚çœ {format_size(total_size - output_size)})")
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    try:
        if progress_file.exists():
            progress_file.unlink()
        if temp_dir.exists() and not any(temp_dir.iterdir()):
            temp_dir.rmdir()
        print("ğŸ§¹ å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶")
    except:
        pass
    
    total_elapsed = time.time() - start_time
    print(f"\nğŸ‰ åˆå¹¶å®Œæˆ! æ€»è€—æ—¶: {total_elapsed:.1f}ç§’")
    print("=" * 50)
    
    return str(output_file)

def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description='åˆå¹¶ FLUX.1-dev åˆ†ç‰‡æ–‡ä»¶ä¸ºå•ä¸ª safetensors æ–‡ä»¶',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python merge_shards.py resources/weights/FLUX.1-dev
  python merge_shards.py resources/weights/FLUX.1-dev --force
        """
    )
    
    parser.add_argument('model_dir', help='æ¨¡å‹ç›®å½•è·¯å¾„ (åŒ…å« transformer/ å­ç›®å½•)')
    parser.add_argument('--force', action='store_true', help='å¼ºåˆ¶é‡æ–°åˆå¹¶ï¼Œè¦†ç›–å·²å­˜åœ¨çš„æ–‡ä»¶')
    parser.add_argument('--no-resume', action='store_true', help='ä¸ä½¿ç”¨æ–­ç‚¹ç»­ä¼ ï¼Œé‡æ–°å¼€å§‹åˆå¹¶')
    parser.add_argument('--check-only', action='store_true', help='ä»…æ£€æŸ¥æ–‡ä»¶çŠ¶æ€ï¼Œä¸æ‰§è¡Œåˆå¹¶')
    
    args = parser.parse_args()
    
    if args.check_only:
        # ä»…æ£€æŸ¥æ¨¡å¼
        model_path = Path(args.model_dir)
        transformer_dir = model_path / "transformer"
        output_file = model_path / "flux1-dev.safetensors"
        
        print("ğŸ” æ–‡ä»¶çŠ¶æ€æ£€æŸ¥:")
        print(f"ğŸ“ æ¨¡å‹ç›®å½•: {model_path} - {'âœ…' if model_path.exists() else 'âŒ'}")
        print(f"ğŸ“‚ Transformerç›®å½•: {'âœ…' if transformer_dir.exists() else 'âŒ'}")
        print(f"ğŸ“¤ åˆå¹¶æ–‡ä»¶: {'âœ…' if output_file.exists() else 'âŒ'}")
        
        if output_file.exists():
            size = format_size(output_file.stat().st_size)
            print(f"ğŸ“ åˆå¹¶æ–‡ä»¶å¤§å°: {size}")
        
        return 0
    
    try:
        output_file = merge_flux_shards(args.model_dir, args.force, resume=not args.no_resume)
        
        if output_file:
            print(f"\nğŸ¯ æˆåŠŸ! åˆå¹¶åçš„æ–‡ä»¶: {output_file}")
            print(f"\nğŸ“ ç°åœ¨å¯ä»¥åœ¨ sample.sh ä¸­è®¾ç½®:")
            print(f'export FLUX_DEV="{output_file}"')
            print(f"\nğŸš€ æˆ–è€…ç›´æ¥è¿è¡Œæµ‹è¯•:")
            print(f'bash sample.sh -m Taylor -i 1 -o 1 -l 1')
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        return 130
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())
