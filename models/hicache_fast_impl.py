from typing import Dict

import math
import torch


def _hicache_polynomial(x: torch.Tensor, n: int) -> torch.Tensor:
    """
    ç‰©ç†å­¦å®¶çš„ Hermite å¤šé¡¹å¼ H_n(x)
    ä½¿ç”¨é€’æ¨å…³ç³»: H_{n+1}(x) = 2x H_n(x) - 2n H_{n-1}(x)
    """
    if n == 0:
        return torch.ones_like(x)
    if n == 1:
        return 2 * x

    H_prev = torch.ones_like(x)
    H_curr = 2 * x

    for k in range(2, n + 1):
        H_next = 2 * x * H_curr - 2 * (k - 1) * H_prev
        H_prev, H_curr = H_curr, H_next

    return H_curr


def _collect_trajectory_feature(cache_dic: Dict, current: Dict, feature: torch.Tensor):
    """
    ç‰¹å¾è½¨è¿¹æ”¶é›†å™¨ - åœ¨ç¼“å­˜æ›´æ–°æ—¶è‡ªåŠ¨æ”¶é›†ç‰¹å¾
    æ”¯æŒå¤šå±‚ã€å¤šæ¨¡å—åŒæ—¶æ”¶é›†ï¼ˆä¸€æ¬¡æ¨ç†æ”¶é›†æ‰€æœ‰éœ€è¦çš„æ¨¡å—ï¼‰

    :param cache_dic: Cache dictionary
    :param current: Information of the current step
    :param feature: Feature tensor to collect
    """
    config = cache_dic.get("feature_collection_config", {})

    # æ”¯æŒå¤šå±‚ç‰¹å¾æ”¶é›†
    target_layers = config.get("target_layers", [14])
    if isinstance(target_layers, int):
        target_layers = [target_layers]  # å‘åå…¼å®¹

    # æ£€æŸ¥æ”¶é›†æ¡ä»¶ - åªæœ‰åœ¨ç›®æ ‡å±‚åˆ—è¡¨ä¸­æ‰æ”¶é›†
    if current["layer"] not in target_layers:
        return

    # ğŸ”¥ æ–°å¢ï¼šæ”¯æŒå¤šæ¨¡å—åŒæ—¶æ”¶é›†
    target_modules = config.get("target_modules", ["any"])
    if isinstance(target_modules, str):
        target_modules = [target_modules]

    # æ£€æŸ¥æ¨¡å—æ˜¯å¦éœ€è¦æ”¶é›†
    if "any" not in target_modules and current["module"] not in target_modules:
        return

    # æ”¯æŒå¤šæµæ”¶é›†
    target_streams = config.get("target_streams", ["any"])
    if isinstance(target_streams, str):
        target_streams = [target_streams]

    # æ£€æŸ¥æµæ˜¯å¦éœ€è¦æ”¶é›†
    if "any" not in target_streams and current["stream"] not in target_streams:
        return

    # ğŸ”¥ æ–°å¢ï¼šæŒ‰å±‚-æ¨¡å—ç»„åˆå­˜å‚¨ï¼Œæ”¯æŒåŒæ—¶æ”¶é›†å¤šä¸ªæ¨¡å—
    if "trajectory_features" not in cache_dic:
        cache_dic["trajectory_features"] = {}
        cache_dic["trajectory_metadata"] = {}

    layer_key = current["layer"]
    module_key = current["module"]

    # åˆå§‹åŒ–å±‚çº§å­˜å‚¨
    if layer_key not in cache_dic["trajectory_features"]:
        cache_dic["trajectory_features"][layer_key] = {}
        cache_dic["trajectory_metadata"][layer_key] = {}

    # åˆå§‹åŒ–æ¨¡å—çº§å­˜å‚¨
    if module_key not in cache_dic["trajectory_features"][layer_key]:
        cache_dic["trajectory_features"][layer_key][module_key] = []
        cache_dic["trajectory_metadata"][layer_key][module_key] = []

    # æ”¶é›†ç‰¹å¾åˆ°å¯¹åº”çš„æ¨¡å—å­˜å‚¨ä¸­
    cache_dic["trajectory_features"][layer_key][module_key].append(feature.clone().detach().cpu())
    cache_dic["trajectory_metadata"][layer_key][module_key].append(
        {
            "step": current["step"],
            "timestep": current.get("t", 0),
            "cache_type": current.get("type", "full"),
            "layer": current["layer"],
            "module": current["module"],
            "stream": current["stream"],
        }
    )


def derivative_approximation(cache_dic: Dict, current: Dict, feature: torch.Tensor):
    """
    Compute derivative approximation.

    :param cache_dic: Cache dictionary
    :param current: Information of the current step
    """
    # ğŸ”¥ æ–°å¢ï¼šç‰¹å¾æ”¶é›†é’©å­
    if cache_dic.get("enable_feature_collection", False):
        _collect_trajectory_feature(cache_dic, current, feature)
        # ğŸ”¥ å¦‚æœåªæ˜¯ä¸ºäº†ç‰¹å¾æ”¶é›†ï¼Œè·³è¿‡å…¶ä½™çš„ç¼“å­˜æ“ä½œ
        if not cache_dic.get("taylor_cache", False):
            return

    # ğŸ”¥ å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿ç¼“å­˜ç»“æ„å­˜åœ¨
    try:
        cache_module = cache_dic["cache"][-1][current["stream"]][current["layer"]][current["module"]]
    except KeyError:
        return

    # ğŸ”¥ å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿æœ‰è¶³å¤Ÿçš„ activated_steps ç”¨äºè®¡ç®—å·®åˆ†
    if len(current["activated_steps"]) < 2:
        # å³ä¾¿å·®åˆ†ä¸å¤Ÿï¼Œä¹Ÿè¦è®°å½•é›¶é˜¶ç‰¹å¾ï¼Œä¾›ä¸‹ä¸€æ¬¡ç¼“å­˜é¢„æµ‹ä½¿ç”¨
        cache_module[0] = feature
        return

    difference_distance = current["activated_steps"][-1] - current["activated_steps"][-2]
    # difference_distance = current['activated_times'][-1] - current['activated_times'][-2]

    updated_taylor_factors = {}
    updated_taylor_factors[0] = feature

    for i in range(cache_dic["max_order"]):
        if (cache_module.get(i, None) is not None) and (current["step"] > cache_dic["first_enhance"] - 2):
            updated_taylor_factors[i + 1] = (updated_taylor_factors[i] - cache_module[i]) / difference_distance
        else:
            break

    cache_dic["cache"][-1][current["stream"]][current["layer"]][current["module"]] = updated_taylor_factors


def get_collected_features(cache_dic: Dict) -> tuple:
    """
    è·å–æ”¶é›†çš„ç‰¹å¾è½¨è¿¹ - æ”¯æŒå¤šå±‚æ•°æ®

    :param cache_dic: Cache dictionary
    :return: (features_dict, metadata_dict) tuple where keys are layer indices
    """
    features = cache_dic.get("trajectory_features", {})
    metadata = cache_dic.get("trajectory_metadata", {})
    return features, metadata


def clear_collected_features(cache_dic: Dict):
    """
    æ¸…ç©ºæ”¶é›†çš„ç‰¹å¾è½¨è¿¹

    :param cache_dic: Cache dictionary
    """
    if "trajectory_features" in cache_dic:
        del cache_dic["trajectory_features"]
    if "trajectory_metadata" in cache_dic:
        del cache_dic["trajectory_metadata"]


def taylor_formula(cache_dic: Dict, current: Dict) -> torch.Tensor:
    """
    Feature prediction dispatcher: chooses between Taylor or HiCache prediction.

    :param cache_dic: Cache dictionary
        - 'prediction_mode': 'taylor' or 'hicache'. Defaults to 'taylor' if not specified.
        - 'use_hicache': (Legacy) If True and 'prediction_mode' is not set, mode becomes 'hicache'.
    :param current: Information of the current step
    """
    # Determine prediction mode, with backward compatibility for 'use_hicache'
    if "prediction_mode" in cache_dic:
        mode = cache_dic["prediction_mode"]
    elif cache_dic.get("use_hicache", False):
        mode = "hicache"
    else:
        mode = "taylor"

    # Dispatch based on mode
    if mode == "taylor":
        return _taylor_expansion_formula(cache_dic, current)
    if mode == "hicache":
        return _hicache_prediction_formula(cache_dic, current)
    if mode == "taylor_scaled":
        return _taylor_scaled_prediction_formula(cache_dic, current)
    raise ValueError(f"Unknown prediction_mode: '{mode}'. Must be 'taylor', 'hicache', or 'taylor_scaled'.")


def _taylor_expansion_formula(cache_dic: Dict, current: Dict) -> torch.Tensor:
    """
    æ ‡å‡†æ³°å‹’å±•å¼€é¢„æµ‹
    ä½¿ç”¨å¹‚å‡½æ•°åŸº: F_pred = F_0 + Î£ (1/k!) * x^k * Î”^kF
    """
    x = current["step"] - current["activated_steps"][-1]
    # x = current['t'] - current['activated_times'][-1]
    output = 0

    # ğŸ”¥ ä¿®å¤ï¼šå®‰å…¨æ£€æŸ¥ï¼Œç¡®ä¿ç¼“å­˜ç»“æ„å­˜åœ¨
    try:
        feats_d = cache_dic["cache"][-1][current["stream"]][current["layer"]][current["module"]]
    except KeyError as exc:
        raise ValueError(
            f"Cache not found for stream='{current['stream']}', layer={current['layer']}, module='{current['module']}'"
        ) from exc

    # ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨max_orderå‚æ•°é™åˆ¶ä½¿ç”¨çš„é¡¹æ•°
    max_order = cache_dic.get("max_order", 3)
    effective_order = min(max_order + 1, len(feats_d))  # +1 because we include 0th order

    for i in range(effective_order):
        output += (1 / math.factorial(i)) * feats_d[i] * (x**i)

    return output


def _hicache_prediction_formula(cache_dic: Dict, current: Dict) -> torch.Tensor:
    """
    åŸºäº Hermite å¤šé¡¹å¼çš„ç‰¹å¾é¢„æµ‹
    """
    x = current["step"] - current["activated_steps"][-1]

    try:
        feats_d = cache_dic["cache"][-1][current["stream"]][current["layer"]][current["module"]]
    except KeyError as exc:
        raise ValueError(
            f"Cache not found for stream='{current['stream']}', layer={current['layer']}, module='{current['module']}'"
        ) from exc

    # ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨max_orderå‚æ•°é™åˆ¶é˜¶æ•°
    max_order = cache_dic.get("max_order", 3)
    available_order = len(feats_d) - 1  # å¯ç”¨é˜¶æ•° = å†å²é¡¹æ•° - 1
    order = min(max_order, available_order)  # ä½¿ç”¨è¾ƒå°å€¼

    if order < 1:
        return feats_d.get(0)  # å†å²ä¸è¶³ï¼Œè¿”å›æœ€æ–°ç‰¹å¾

    F_latest = feats_d[0].clone()  # F_0

    # å°†æ—¶é—´æ­¥å·®å€¼è½¬æ¢ä¸ºtensorï¼Œä¿æŒä¸ç‰¹å¾ç›¸åŒçš„dtypeå’Œdevice
    x_tensor = torch.tensor(float(x), dtype=F_latest.dtype, device=F_latest.device)

    # è·å–ç¼©æ”¾å› å­ï¼Œç”¨äºæ§åˆ¶ Hermite å¤šé¡¹å¼çš„å¢é•¿
    scale_factor = cache_dic.get("hicache_scale_factor", 0.5)
    x_scaled = x_tensor * scale_factor

    # æ„é€  Hermite é¢„æµ‹
    pred = F_latest.clone()

    for k in range(1, order + 1):
        diff_k = feats_d[k]
        Hk = _hicache_polynomial(x_scaled, k)
        # è€ƒè™‘ç¼©æ”¾å› å­çš„å½±å“
        alpha = float(Hk / math.factorial(k)) * (scale_factor**k)
        pred.add_(diff_k, alpha=alpha)

    return pred


def _taylor_scaled_prediction_formula(cache_dic: Dict, current: Dict) -> torch.Tensor:
    """
    Taylor é¢„æµ‹çš„â€œåŒé‡ç¼©æ”¾â€å˜ä½“ã€‚
    """
    x = current["step"] - current["activated_steps"][-1]

    try:
        feats_d = cache_dic["cache"][-1][current["stream"]][current["layer"]][current["module"]]
    except KeyError as exc:
        raise ValueError(
            f"Cache not found for stream='{current['stream']}', layer={current['layer']}, module='{current['module']}'"
        ) from exc

    max_order = cache_dic.get("max_order", 3)
    available_order = len(feats_d) - 1
    order = min(max_order, available_order)
    if order < 1:
        return feats_d.get(0)

    F_latest = feats_d[0].clone()
    x_tensor = torch.tensor(float(x), dtype=F_latest.dtype, device=F_latest.device)
    scale = cache_dic.get("hicache_scale_factor", 0.5)

    pred = F_latest.clone()
    for k in range(1, order + 1):
        diff_k = feats_d[k]
        # åŒé‡ç¼©æ”¾ï¼šç³»æ•° = (1/k!) * (s^(2k)) * (x^k)
        alpha = (float(x_tensor**k) / math.factorial(k)) * (scale ** (2 * k))
        pred.add_(diff_k, alpha=alpha)

    return pred


def taylor_cache_init(cache_dic: Dict, current: Dict):
    """
    Initialize Taylor cache and allocate storage for different-order derivatives in the Taylor cache.

    :param cache_dic: Cache dictionary
    :param current: Information of the current step
    """
    if (current["step"] == 0) and (cache_dic["taylor_cache"]):
        cache_dic["cache"][-1][current["stream"]][current["layer"]][current["module"]] = {}

