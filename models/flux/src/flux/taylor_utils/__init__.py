from typing import Dict
import torch
import math


def _hicache_polynomial(x: torch.Tensor, n: int) -> torch.Tensor:
    """
    ç‰©ç†å­¦å®¶çš„ Hermite å¤šé¡¹å¼ H_n(x)
    ä½¿ç”¨é€’æ¨å…³ç³»: H_{n+1}(x) = 2x H_n(x) - 2n H_{n-1}(x)
    """
    if n == 0:
        return torch.ones_like(x)
    elif n == 1:
        return 2 * x

    H_prev = torch.ones_like(x)
    H_curr = 2 * x

    for k in range(2, n + 1):
        H_next = 2 * x * H_curr - 2 * (k - 1) * H_prev
        H_prev, H_curr = H_curr, H_next

    return H_curr


def _update_analytic_stats(cache_dic: Dict, layer_key: int, delta_tensor: torch.Tensor):
    """
    Update analytic statistics for adaptive sigma calculation.
    """
    if not cache_dic.get("analytic_sigma", False):
        return

    # Get config
    config = cache_dic.get("analytic_sigma_config", {})
    beta = config.get("beta", 0.01)

    # beta <= 0: è§†ä¸ºâ€œå…³é—­åœ¨çº¿æ›´æ–°â€ï¼Œä¿æŒè§£æ Ïƒ ä½¿ç”¨é»˜è®¤ q=1
    # å³ä¸å†™å…¥ä»»ä½• analytic_statsï¼Œè®© _compute_analytic_sigma èµ°é»˜è®¤åˆ†æ”¯ã€‚
    if beta <= 0:
        return

    # Calculate scalar scale D
    # delta_tensor shape: [Batch, Sequence, Dim] or [Sequence, Dim]
    # We want a robust measure of magnitude.
    with torch.no_grad():
        norms = delta_tensor.detach().float().norm(dim=-1)
        q_quantile = config.get("q_quantile")
        if q_quantile is not None and 0.0 < float(q_quantile) < 1.0:
            # ä½¿ç”¨åˆ†ä½æ•°ä»¥æŠµæŠ—å°‘é‡ outlier
            D_val = torch.quantile(norms, float(q_quantile)).item()
        else:
            D_val = norms.mean().item()

    # Initialize storage if needed
    if "analytic_stats" not in cache_dic:
        cache_dic["analytic_stats"] = {}

    stats = cache_dic["analytic_stats"]

    if layer_key not in stats:
        # Initialize
        stats[layer_key] = D_val
    else:
        # EMA update: q = sqrt( (1-beta)*q^2 + beta*D^2 )
        q_old = stats[layer_key]
        # Use squared average EMA for RMS-like behavior
        q_sq_old = q_old**2
        q_sq_new = (1 - beta) * q_sq_old + beta * (D_val**2)
        stats[layer_key] = math.sqrt(q_sq_new)


def _compute_analytic_sigma(cache_dic: Dict, layer_key: int) -> float:
    """
    Compute analytic sigma based on collected statistics.
    """
    stats = cache_dic.get("analytic_stats", {})
    # If no stats collected yet (e.g. first step), fallback to a default scale
    # Or use a safe default like 1.0 for q, which implies sigma ~ alpha/1.414
    q = stats.get(layer_key, 1.0)

    config = cache_dic.get("analytic_sigma_config", {})
    alpha = config.get("alpha", 1.28)
    sigma_max = config.get("sigma_max", 1.0)
    eps = config.get("eps", 1e-6)
    sigma_smooth = config.get("sigma_smooth", 0.0)

    # Formula: sigma = min(sigma_max, alpha / (sqrt(2)*q + eps))
    raw = alpha / (math.sqrt(2) * q + eps)
    sigma_new = min(sigma_max, raw)

    # å¯é€‰ï¼šåœ¨ log åŸŸå¯¹ sigma åšå¹³æ»‘ï¼Œé¿å…ç›¸é‚»æ­¥è·³å˜
    if sigma_smooth and sigma_smooth > 0:
        state = cache_dic.setdefault("analytic_sigma_state", {})
        prev = state.get(layer_key, sigma_new)
        gamma = float(sigma_smooth)
        log_prev = math.log(max(prev, eps))
        log_new = math.log(max(sigma_new, eps))
        smoothed = math.exp((1 - gamma) * log_prev + gamma * log_new)
        sigma_new = min(sigma_max, smoothed)
        state[layer_key] = sigma_new

    return sigma_new



def _collect_trajectory_feature(cache_dic: Dict, current: Dict, feature: torch.Tensor):
    """
    ç‰¹å¾è½¨è¿¹æ”¶é›†å™¨ - åœ¨ç¼“å­˜æ›´æ–°æ—¶è‡ªåŠ¨æ”¶é›†ç‰¹å¾
    æ”¯æŒå¤šå±‚ã€å¤šæ¨¡å—åŒæ—¶æ”¶é›†ï¼ˆä¸€æ¬¡æ¨ç†æ”¶é›†æ‰€æœ‰éœ€è¦çš„æ¨¡å—ï¼‰

    :param cache_dic: Cache dictionary
    :param current: Information of the current step
    :param feature: Feature tensor to collect
    """
    config = cache_dic.get("feature_collection_config", {})

    # æ”¯æŒå¤šå±‚ç‰¹å¾æ”¶é›†
    target_layers = config.get("target_layers", [14])
    if isinstance(target_layers, int):
        target_layers = [target_layers]  # å‘åå…¼å®¹

    # æ£€æŸ¥æ”¶é›†æ¡ä»¶ - åªæœ‰åœ¨ç›®æ ‡å±‚åˆ—è¡¨ä¸­æ‰æ”¶é›†
    if current["layer"] not in target_layers:
        return

    # ğŸ”¥ æ–°å¢ï¼šæ”¯æŒå¤šæ¨¡å—åŒæ—¶æ”¶é›†
    target_modules = config.get("target_modules", ["any"])
    if isinstance(target_modules, str):
        target_modules = [target_modules]

    # æ£€æŸ¥æ¨¡å—æ˜¯å¦éœ€è¦æ”¶é›†
    if "any" not in target_modules and current["module"] not in target_modules:
        return

    # æ”¯æŒå¤šæµæ”¶é›†
    target_streams = config.get("target_streams", ["any"])
    if isinstance(target_streams, str):
        target_streams = [target_streams]

    # æ£€æŸ¥æµæ˜¯å¦éœ€è¦æ”¶é›†
    if "any" not in target_streams and current["stream"] not in target_streams:
        return

    # ğŸ”¥ æ–°å¢ï¼šæŒ‰å±‚-æ¨¡å—ç»„åˆå­˜å‚¨ï¼Œæ”¯æŒåŒæ—¶æ”¶é›†å¤šä¸ªæ¨¡å—
    if "trajectory_features" not in cache_dic:
        cache_dic["trajectory_features"] = {}
        cache_dic["trajectory_metadata"] = {}

    layer_key = current["layer"]
    module_key = current["module"]

    # åˆå§‹åŒ–å±‚çº§å­˜å‚¨
    if layer_key not in cache_dic["trajectory_features"]:
        cache_dic["trajectory_features"][layer_key] = {}
        cache_dic["trajectory_metadata"][layer_key] = {}

    # åˆå§‹åŒ–æ¨¡å—çº§å­˜å‚¨
    if module_key not in cache_dic["trajectory_features"][layer_key]:
        cache_dic["trajectory_features"][layer_key][module_key] = []
        cache_dic["trajectory_metadata"][layer_key][module_key] = []

    # æ”¶é›†ç‰¹å¾åˆ°å¯¹åº”çš„æ¨¡å—å­˜å‚¨ä¸­
    cache_dic["trajectory_features"][layer_key][module_key].append(feature.clone().detach().cpu())
    cache_dic["trajectory_metadata"][layer_key][module_key].append(
        {
            "step": current["step"],
            "timestep": current.get("t", 0),
            "cache_type": current.get("type", "full"),
            "layer": current["layer"],
            "module": current["module"],
            "stream": current["stream"],
        }
    )

def derivative_approximation(cache_dic: Dict, current: Dict, feature: torch.Tensor):
    """
    Compute derivative approximation.

    :param cache_dic: Cache dictionary
    :param current: Information of the current step
    """
    # ğŸ”¥ æ–°å¢ï¼šç‰¹å¾æ”¶é›†é’©å­
    if cache_dic.get("enable_feature_collection", False):
        _collect_trajectory_feature(cache_dic, current, feature)
        # ğŸ”¥ å¦‚æœåªæ˜¯ä¸ºäº†ç‰¹å¾æ”¶é›†ï¼Œè·³è¿‡å…¶ä½™çš„ç¼“å­˜æ“ä½œ
        if not cache_dic.get("taylor_cache", False):
            return

    # ğŸ”¥ å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿ç¼“å­˜ç»“æ„å­˜åœ¨
    try:
        cache_module = cache_dic["cache"][-1][current["stream"]][current["layer"]][current["module"]]
    except KeyError:
        return

    # ğŸ”¥ å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿æœ‰è¶³å¤Ÿçš„ activated_steps ç”¨äºè®¡ç®—å·®åˆ†
    if len(current["activated_steps"]) < 2:
        # å³ä¾¿å·®åˆ†ä¸å¤Ÿï¼Œä¹Ÿè¦è®°å½•é›¶é˜¶ç‰¹å¾ï¼Œä¾›ä¸‹ä¸€æ¬¡ç¼“å­˜é¢„æµ‹ä½¿ç”¨
        cache_module[0] = feature
        return

    difference_distance = current["activated_steps"][-1] - current["activated_steps"][-2]
    # difference_distance = current['activated_times'][-1] - current['activated_times'][-2]

    updated_taylor_factors = {}
    updated_taylor_factors[0] = feature

    for i in range(cache_dic["max_order"]):
        if (cache_module.get(i, None) is not None) and (current["step"] > cache_dic["first_enhance"] - 2):
            updated_taylor_factors[i + 1] = (
                updated_taylor_factors[i] - cache_module[i]
            ) / difference_distance
            
            # ğŸ”¥ Analytic Sigma Update: Capture first-order difference (delta_F)
            if i == 0:
                _update_analytic_stats(cache_dic, current["layer"], updated_taylor_factors[1])
                
        else:
            break

    cache_dic["cache"][-1][current["stream"]][current["layer"]][current["module"]] = updated_taylor_factors


def get_collected_features(cache_dic: Dict) -> tuple:
    """
    è·å–æ”¶é›†çš„ç‰¹å¾è½¨è¿¹ - æ”¯æŒå¤šå±‚æ•°æ®

    :param cache_dic: Cache dictionary
    :return: (features_dict, metadata_dict) tuple where keys are layer indices
    """
    features = cache_dic.get("trajectory_features", {})
    metadata = cache_dic.get("trajectory_metadata", {})
    return features, metadata


def clear_collected_features(cache_dic: Dict):
    """
    æ¸…ç©ºæ”¶é›†çš„ç‰¹å¾è½¨è¿¹

    :param cache_dic: Cache dictionary
    """
    if "trajectory_features" in cache_dic:
        del cache_dic["trajectory_features"]
    if "trajectory_metadata" in cache_dic:
        del cache_dic["trajectory_metadata"]


def taylor_formula(cache_dic: Dict, current: Dict) -> torch.Tensor:
    """
    Feature prediction dispatcher: chooses between Taylor or HiCache prediction.

    :param cache_dic: Cache dictionary
        - 'prediction_mode': 'taylor' or 'hicache'. Defaults to 'taylor' if not specified.
        - 'use_hicache': (Legacy) If True and 'prediction_mode' is not set, mode becomes 'hicache'.
    :param current: Information of the current step
    """
    # Determine prediction mode, with backward compatibility for 'use_hicache'
    if "prediction_mode" in cache_dic:
        mode = cache_dic["prediction_mode"]
    elif cache_dic.get("use_hicache", False):
        mode = "hicache"
    else:
        mode = "taylor"

    # Dispatch based on mode
    if mode == "taylor":
        return _taylor_expansion_formula(cache_dic, current)

    elif mode == "hicache":
        return _hicache_prediction_formula(cache_dic, current)

    elif mode == "taylor_scaled":
        return _taylor_scaled_prediction_formula(cache_dic, current)

    else:
        raise ValueError(
            f"Unknown prediction_mode: '{mode}'. Must be 'taylor', 'hicache', or 'taylor_scaled'."
        )


def _taylor_expansion_formula(cache_dic: Dict, current: Dict) -> torch.Tensor:
    """
    æ ‡å‡†æ³°å‹’å±•å¼€é¢„æµ‹
    ä½¿ç”¨å¹‚å‡½æ•°åŸº: F_pred = F_0 + Î£ (1/k!) * x^k * Î”^kF
    """
    x = current["step"] - current["activated_steps"][-1]
    # x = current['t'] - current['activated_times'][-1]
    output = 0

    # ğŸ”¥ ä¿®å¤ï¼šå®‰å…¨æ£€æŸ¥ï¼Œç¡®ä¿ç¼“å­˜ç»“æ„å­˜åœ¨
    try:
        feats_d = cache_dic["cache"][-1][current["stream"]][current["layer"]][current["module"]]
    except KeyError:
        # ğŸ”¥ ä¿®å¤ï¼šå¦‚æœç¼“å­˜ä¸å­˜åœ¨ï¼Œè¯´æ˜è¿™æ˜¯ç¬¬ä¸€æ¬¡è®¿é—®è¯¥æ¨¡å—
        # åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åº”è¯¥åˆå§‹åŒ–ç¼“å­˜ç»“æ„å¹¶è¿”å›é›¶å¼ é‡
        if current["stream"] not in cache_dic["cache"][-1]:
            cache_dic["cache"][-1][current["stream"]] = {}
        if current["layer"] not in cache_dic["cache"][-1][current["stream"]]:
            cache_dic["cache"][-1][current["stream"]][current["layer"]] = {}
        if current["module"] not in cache_dic["cache"][-1][current["stream"]][current["layer"]]:
            cache_dic["cache"][-1][current["stream"]][current["layer"]][current["module"]] = {}

        # ğŸ”¥ ä¿®å¤ï¼šå¦‚æœç¼“å­˜ä¸ºç©ºï¼Œè¿™æ„å‘³ç€è¿™æ˜¯ç¬¬ä¸€æ¬¡è°ƒç”¨è¯¥æ¨¡å—
        # åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åº”è¯¥æŠ›å‡ºä¸€ä¸ªæ›´å‹å¥½çš„é”™è¯¯ä¿¡æ¯ï¼ŒæŒ‡å¯¼ç”¨æˆ·æ£€æŸ¥é…ç½®
        raise ValueError(
            f"Cache not found for stream='{current['stream']}', layer={current['layer']}, module='{current['module']}'. "
            f"This usually means the first step was not run in 'full' mode to initialize the cache. "
            f"Please check your cache configuration and ensure first_enhance >= 1."
        )

    # ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨max_orderå‚æ•°é™åˆ¶ä½¿ç”¨çš„é¡¹æ•°
    max_order = cache_dic.get("max_order", 3)
    effective_order = min(max_order + 1, len(feats_d))  # +1 because we include 0th order

    for i in range(effective_order):
        output += (1 / math.factorial(i)) * feats_d[i] * (x**i)

    return output


def _hicache_prediction_formula(cache_dic: Dict, current: Dict) -> torch.Tensor:
    """
    åŸºäº Hermite å¤šé¡¹å¼çš„ç‰¹å¾é¢„æµ‹

    ä½¿ç”¨ Hermite å¤šé¡¹å¼ä½œä¸ºåŸºå‡½æ•°ï¼Œè€Œä¸æ˜¯æ ‡å‡†çš„å¹‚å‡½æ•°ï¼š
    F_pred = F_0 + Î£_{k=1}^{n} (1/k!) * H_k(x) * Î”^kF

    å…¶ä¸­ï¼š
    - H_k(x) æ˜¯ k é˜¶ Hermite å¤šé¡¹å¼
    - Î”^kF æ˜¯ k é˜¶å·®åˆ†ç‰¹å¾
    - x æ˜¯æ—¶é—´æ­¥å·®å€¼

    è¿™ç§æ–¹æ³•çš„ä¼˜åŠ¿ï¼š
    1. Hermite å¤šé¡¹å¼å…·æœ‰æ­£äº¤æ€§ï¼Œæ•°å€¼ç¨³å®šæ€§æ›´å¥½
    2. åœ¨æŸäº›å‡½æ•°ç±»å‹ä¸Šé€¼è¿‘ç²¾åº¦æ›´é«˜
    3. é€šè¿‡ç¼©æ”¾å› å­å¯ä»¥æ§åˆ¶å¤šé¡¹å¼å¢é•¿
    """
    # è·å–å®é™…çš„æ—¶é—´æ­¥å·®å€¼ï¼Œä¿æŒä¸åŸå§‹æ³°å‹’å±•å¼€çš„ä¸€è‡´æ€§
    x = current["step"] - current["activated_steps"][-1]
    # å¦‚æœéœ€è¦ä½¿ç”¨æ—¶é—´å·®å€¼ï¼Œå¯ä»¥å–æ¶ˆæ³¨é‡Šä¸‹é¢è¿™è¡Œ
    # x = current['t'] - current['activated_times'][-1]

    # è·å–ç‰¹å¾ç¼“å­˜
    try:
        feats_d = cache_dic["cache"][-1][current["stream"]][current["layer"]][current["module"]]
    except KeyError:
        raise ValueError(
            f"Cache not found for stream='{current['stream']}', layer={current['layer']}, module='{current['module']}'"
        )

    # ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨max_orderå‚æ•°é™åˆ¶é˜¶æ•°
    max_order = cache_dic.get("max_order", 3)
    available_order = len(feats_d) - 1  # å¯ç”¨é˜¶æ•° = å†å²é¡¹æ•° - 1
    order = min(max_order, available_order)  # ä½¿ç”¨è¾ƒå°å€¼

    if order < 1:
        return feats_d.get(0)  # å†å²ä¸è¶³ï¼Œè¿”å›æœ€æ–°ç‰¹å¾

    F_latest = feats_d[0].clone()  # F_0

    # å°†æ—¶é—´æ­¥å·®å€¼è½¬æ¢ä¸ºtensorï¼Œä¿æŒä¸ç‰¹å¾ç›¸åŒçš„dtypeå’Œdevice
    x_tensor = torch.tensor(float(x), dtype=F_latest.dtype, device=F_latest.device)

    # è·å–ç¼©æ”¾å› å­ï¼Œç”¨äºæ§åˆ¶ Hermite å¤šé¡¹å¼çš„å¢é•¿
    if cache_dic.get("analytic_sigma", False):
        scale_factor = _compute_analytic_sigma(cache_dic, current["layer"])
    else:
        scale_factor = cache_dic.get("hicache_scale_factor", 0.5)
        
    x_scaled = x_tensor * scale_factor

    # æ„é€  Hermite é¢„æµ‹
    pred = F_latest.clone()

    for k in range(1, order + 1):
        diff_k = feats_d[k]
        Hk = _hicache_polynomial(x_scaled, k)
        # è€ƒè™‘ç¼©æ”¾å› å­çš„å½±å“
        alpha = float(Hk / math.factorial(k)) * (scale_factor**k)
        pred.add_(diff_k, alpha=alpha)

    return pred


def _taylor_scaled_prediction_formula(cache_dic: Dict, current: Dict) -> torch.Tensor:
    """
    Taylor é¢„æµ‹çš„â€œåŒé‡ç¼©æ”¾â€å˜ä½“ï¼š

    åœ¨æ ‡å‡†å¹‚å‡½æ•°åŸºçš„åŸºç¡€ä¸Šè¿›è¡ŒåŒé‡ç¼©æ”¾ï¼š
      F_pred = F_0 + Î£_{k=1..n} (1/k!) * (s x)^k * (s^k) * Î”^kF
             = F_0 + Î£_{k=1..n} (1/k!) * (s^(2k)) * (x^k) * Î”^kF

    å…¶ä¸­ s = hicache_scale_factor, x ä¸ºæ­¥è·ï¼ˆå½“å‰æ­¥ä¸æœ€è¿‘ä¸€æ¬¡ full æ­¥çš„å·®ï¼‰ã€‚
    è¯¥å½¢å¼ç”¨äºå¯¹æ¯” Hermite åŒé‡ç¼©æ”¾ä¸æ™®é€šå¤šé¡¹å¼åŸºçš„æ•ˆæœå·®å¼‚ã€‚
    """
    x = current["step"] - current["activated_steps"][-1]

    try:
        feats_d = cache_dic["cache"][-1][current["stream"]][current["layer"]][current["module"]]
    except KeyError:
        raise ValueError(
            f"Cache not found for stream='{current['stream']}', layer={current['layer']}, module='{current['module']}'"
        )

    max_order = cache_dic.get("max_order", 3)
    available_order = len(feats_d) - 1
    order = min(max_order, available_order)
    if order < 1:
        return feats_d.get(0)

    F_latest = feats_d[0].clone()
    x_tensor = torch.tensor(float(x), dtype=F_latest.dtype, device=F_latest.device)
    scale = cache_dic.get("hicache_scale_factor", 0.5)

    pred = F_latest.clone()
    for k in range(1, order + 1):
        diff_k = feats_d[k]
        # åŒé‡ç¼©æ”¾ï¼šç³»æ•° = (1/k!) * (s^(2k)) * (x^k)
        alpha = (float(x_tensor**k) / math.factorial(k)) * (scale ** (2 * k))
        pred.add_(diff_k, alpha=alpha)

    return pred


def taylor_cache_init(cache_dic: Dict, current: Dict):
    """
    Initialize Taylor cache and allocate storage for different-order derivatives in the Taylor cache.

    :param cache_dic: Cache dictionary
    :param current: Information of the current step
    """
    if (current["step"] == 0) and (cache_dic["taylor_cache"]):
        cache_dic["cache"][-1][current["stream"]][current["layer"]][current["module"]] = {}
